{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import json\n",
    "from scipy.sparse import csr_matrix, vstack\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from enum import Enum\n",
    "from dataclasses import dataclass\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InteractionTypes(Enum):\n",
    "    VIEW = \"view\"\n",
    "    LIKE = \"like\"\n",
    "    SHARE = \"share\"\n",
    "    NOT_INTERESTED = \"not_interested\"\n",
    "    \n",
    "    @classmethod\n",
    "    def from_string(cls, string_value: str) -> 'InteractionTypes':\n",
    "        \"\"\"Convert string to InteractionTypes enum value\"\"\"\n",
    "        try:\n",
    "            return cls(string_value.lower())\n",
    "        except ValueError:\n",
    "            raise ValueError(f\"Invalid interaction type. Must be one of: {[e.value for e in cls]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ArticleBoost:\n",
    "    article_id: int\n",
    "    boost_factor: float\n",
    "    start_time: datetime\n",
    "    end_time: datetime\n",
    "    boost_type: str  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedHybridRecommender:\n",
    "    def __init__(self, article_data: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Initialize the enhanced hybrid recommender system\n",
    "        \n",
    "        Args:\n",
    "            article_data: DataFrame with columns ['title', 'text', 'authors', 'timestamp', 'tags']\n",
    "        \"\"\"\n",
    "        self.articles = article_data\n",
    "        self.article_ids = list(range(len(article_data)))\n",
    "        \n",
    "        # Interaction weights for different types\n",
    "        self.interaction_weights = {\n",
    "            InteractionTypes.VIEW: 0.3,\n",
    "            InteractionTypes.LIKE: 1.0,\n",
    "            InteractionTypes.SHARE: 1.5,\n",
    "            InteractionTypes.NOT_INTERESTED: -2.0\n",
    "        }\n",
    "        \n",
    "        # Initialize matrices for different interaction types\n",
    "        self.interaction_matrices = {\n",
    "            interaction_type: csr_matrix((0, len(self.articles)))\n",
    "            for interaction_type in InteractionTypes\n",
    "        }\n",
    "        \n",
    "        # User preferences and article boosting\n",
    "        self.user_preferences = defaultdict(lambda: defaultdict(float))\n",
    "        self.article_boosts = []\n",
    "        self.user_negative_feedback = defaultdict(set)\n",
    "        \n",
    "        # Content-based components\n",
    "        self.content_similarities = None\n",
    "        self.tfidf_vectorizer = None\n",
    "        self.article_features = None\n",
    "        self._prepare_content_features()\n",
    "        \n",
    "        # Collaborative filtering components\n",
    "        self.collaborative_model = None\n",
    "        self.user_embeddings = None\n",
    "        self.article_embeddings = None\n",
    "        \n",
    "        # Initialize topic modeling\n",
    "        self._initialize_topic_modeling()\n",
    "        \n",
    "    def _initialize_topic_modeling(self):\n",
    "        \"\"\"Initialize topic modeling for better content understanding\"\"\"\n",
    "        # Using SVD for topic modeling\n",
    "        self.topic_model = TruncatedSVD(n_components=50)\n",
    "        self.topic_features = self.topic_model.fit_transform(self.article_features)\n",
    "        \n",
    "    def _prepare_content_features(self):\n",
    "        \"\"\"Prepare enhanced content features using TF-IDF and metadata\"\"\"\n",
    "        # Combine text, tags, and authors with weights\n",
    "        self.articles['processed_content'] = self.articles.apply(\n",
    "            lambda x: (\n",
    "                f\"{x['text']} \" + \n",
    "                f\"{' '.join(x['tags'] * 3)} \" +  # Increase tag importance\n",
    "                f\"{' '.join(x['authors'] * 2)}\"  # Increase author importance\n",
    "            ),\n",
    "            axis=1\n",
    "        )\n",
    "        \n",
    "        # Create TF-IDF features with enhanced parameters\n",
    "        self.tfidf_vectorizer = TfidfVectorizer(\n",
    "            max_features=5000,\n",
    "            stop_words='english',\n",
    "            ngram_range=(1, 2)  # Include bigrams\n",
    "        )\n",
    "        self.article_features = self.tfidf_vectorizer.fit_transform(\n",
    "            self.articles['processed_content']\n",
    "        )\n",
    "        \n",
    "        # Calculate content-based similarity matrix\n",
    "        self.content_similarities = cosine_similarity(self.article_features)\n",
    "        \n",
    "    def add_article_boost(self, boost: ArticleBoost):\n",
    "        \"\"\"Add a boost to an article\"\"\"\n",
    "        self.article_boosts.append(boost)\n",
    "        # Clean expired boosts\n",
    "        current_time = datetime.now()\n",
    "        self.article_boosts = [\n",
    "            b for b in self.article_boosts \n",
    "            if b.end_time > current_time\n",
    "        ]\n",
    "        \n",
    "    def add_user_interaction(\n",
    "        self, \n",
    "        user_id: int, \n",
    "        article_id: int, \n",
    "        interaction_type: str,\n",
    "        timestamp: Optional[datetime] = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Add a user interaction with type-specific handling\n",
    "        \n",
    "        Args:\n",
    "            user_id: unique identifier for the user\n",
    "            article_id: index of the article in self.articles\n",
    "            interaction_type: type of interaction ('view', 'like', 'share', 'not_interested')\n",
    "            timestamp: timestamp of the interaction\n",
    "        \"\"\"\n",
    "        # Convert string interaction type to enum\n",
    "        try:\n",
    "            interaction_enum = InteractionTypes.from_string(interaction_type)\n",
    "        except ValueError as e:\n",
    "            raise ValueError(str(e))\n",
    "            \n",
    "        matrix = self.interaction_matrices[interaction_enum]\n",
    "        \n",
    "        # Expand matrix if needed\n",
    "        if user_id >= matrix.shape[0]:\n",
    "            new_rows = user_id - matrix.shape[0] + 1\n",
    "            additional_matrix = csr_matrix((new_rows, matrix.shape[1]))\n",
    "            self.interaction_matrices[interaction_enum] = vstack([matrix, additional_matrix])\n",
    "            matrix = self.interaction_matrices[interaction_enum]\n",
    "        \n",
    "        # Update interaction matrix\n",
    "        matrix[user_id, article_id] = 1\n",
    "        \n",
    "        # Update user preferences\n",
    "        weight = self.interaction_weights[interaction_enum]\n",
    "        self.user_preferences[user_id][article_id] += weight\n",
    "        \n",
    "        # Handle negative feedback\n",
    "        if interaction_enum == InteractionTypes.NOT_INTERESTED:\n",
    "            self.user_negative_feedback[user_id].add(article_id)\n",
    "            # Add similar articles to negative feedback with reduced weight\n",
    "            similar_articles = self.get_similar_articles(article_id, n_recommendations=5)\n",
    "            for similar in similar_articles:\n",
    "                self.user_preferences[user_id][similar['article_id']] += weight * 0.3\n",
    "        \n",
    "        # Update collaborative model if enough interactions\n",
    "        if sum(matrix.getnnz() for matrix in self.interaction_matrices.values()) > 100:\n",
    "            self._update_collaborative_model()\n",
    "            \n",
    "    def _update_collaborative_model(self):\n",
    "        \"\"\"Update the collaborative filtering model using weighted interactions\"\"\"\n",
    "        # Combine interaction matrices with weights\n",
    "        combined_matrix = csr_matrix(self.interaction_matrices[InteractionTypes.VIEW].shape)\n",
    "        \n",
    "        for int_type, matrix in self.interaction_matrices.items():\n",
    "            weight = self.interaction_weights[int_type]\n",
    "            combined_matrix += matrix * weight\n",
    "        \n",
    "        # Apply matrix factorization\n",
    "        self.collaborative_model = TruncatedSVD(n_components=50)\n",
    "        self.user_embeddings = self.collaborative_model.fit_transform(combined_matrix)\n",
    "        self.article_embeddings = self.collaborative_model.components_.T\n",
    "        \n",
    "    def _get_article_boost_score(self, article_id: int) -> float:\n",
    "        \"\"\"Calculate current boost score for an article\"\"\"\n",
    "        current_time = datetime.now()\n",
    "        boost_score = 1.0\n",
    "        \n",
    "        for boost in self.article_boosts:\n",
    "            if (boost.article_id == article_id and \n",
    "                boost.start_time <= current_time <= boost.end_time):\n",
    "                boost_score *= boost.boost_factor\n",
    "        \n",
    "        return boost_score\n",
    "    \n",
    "    def get_recommendations(\n",
    "        self, \n",
    "        user_id: Optional[int] = None, \n",
    "        article_id: Optional[int] = None,\n",
    "        n_recommendations: int = 5,\n",
    "        include_boosted: bool = True\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Get enhanced hybrid recommendations\n",
    "        \n",
    "        Args:\n",
    "            user_id: ID of user to get recommendations for (optional)\n",
    "            article_id: ID of article to get similar articles for (optional)\n",
    "            n_recommendations: number of recommendations to return\n",
    "            include_boosted: whether to include boosted articles\n",
    "        \"\"\"\n",
    "        content_scores = np.zeros(len(self.articles))\n",
    "        collaborative_scores = np.zeros(len(self.articles))\n",
    "        \n",
    "        # Get content-based scores\n",
    "        if article_id is not None:\n",
    "            content_scores = self.content_similarities[article_id]\n",
    "            \n",
    "        # Get collaborative filtering scores\n",
    "        if user_id is not None and self.collaborative_model is not None:\n",
    "            if user_id < self.user_embeddings.shape[0]:\n",
    "                user_vector = self.user_embeddings[user_id]\n",
    "                collaborative_scores = np.dot(user_vector, self.article_embeddings.T)\n",
    "        \n",
    "        # Combine scores with weights that depend on available data\n",
    "        if self.collaborative_model is not None:\n",
    "            # More weight to collaborative as we have more user data\n",
    "            interaction_count = sum(matrix.getnnz() for matrix in self.interaction_matrices.values())\n",
    "            collab_weight = min(0.8, interaction_count / 1000)  # Cap at 0.8\n",
    "            content_weight = 1 - collab_weight\n",
    "            final_scores = (content_weight * content_scores + \n",
    "                          collab_weight * collaborative_scores)\n",
    "        else:\n",
    "            final_scores = content_scores\n",
    "        \n",
    "        # Apply article boosting\n",
    "        if include_boosted:\n",
    "            boost_scores = np.array([\n",
    "                self._get_article_boost_score(aid) \n",
    "                for aid in range(len(self.articles))\n",
    "            ])\n",
    "            final_scores *= boost_scores\n",
    "        \n",
    "        # Filter out articles with negative feedback\n",
    "        if user_id is not None:\n",
    "            negative_articles = self.user_negative_feedback[user_id]\n",
    "            final_scores[list(negative_articles)] = float('-inf')\n",
    "        \n",
    "        # Get top recommendations\n",
    "        recommended_ids = np.argsort(final_scores)[::-1][:n_recommendations]\n",
    "        \n",
    "        recommendations = []\n",
    "        for idx in recommended_ids:\n",
    "            recommendations.append({\n",
    "                'article_id': idx,\n",
    "                'title': self.articles.iloc[idx]['title'],\n",
    "                'score': float(final_scores[idx]),\n",
    "                'tags': self.articles.iloc[idx]['tags'],\n",
    "                'boost_score': float(self._get_article_boost_score(idx)),\n",
    "                'is_boosted': self._get_article_boost_score(idx) > 1.0\n",
    "            })\n",
    "        \n",
    "        return recommendations\n",
    "    \n",
    "    def get_user_profile(self, user_id: int) -> Dict:\n",
    "        \"\"\"Get a user's preference profile\"\"\"\n",
    "        # Check if user_id is out of range for the interaction matrices\n",
    "        max_user_id = max(matrix.shape[0] for matrix in self.interaction_matrices.values())\n",
    "        if user_id >= max_user_id:\n",
    "            return {\n",
    "                'total_interactions': 0,\n",
    "                'favorite_tags': [],\n",
    "                'interaction_counts': {int_type.name: 0 for int_type in InteractionTypes}\n",
    "            }\n",
    "                \n",
    "        # Count interactions by type safely\n",
    "        interaction_counts = {}\n",
    "        for int_type in InteractionTypes:\n",
    "            matrix = self.interaction_matrices[int_type]\n",
    "            if user_id < matrix.shape[0]:\n",
    "                # Get the user's row as a sparse matrix\n",
    "                user_row = matrix[user_id]\n",
    "                interaction_counts[int_type.name] = user_row.getnnz()\n",
    "            else:\n",
    "                interaction_counts[int_type.name] = 0\n",
    "            \n",
    "        # Get favorite tags from positive interactions\n",
    "        user_articles = [\n",
    "            aid for aid, score in self.user_preferences[user_id].items()\n",
    "            if score > 0\n",
    "        ]\n",
    "        \n",
    "        tag_counts = defaultdict(int)\n",
    "        for aid in user_articles:\n",
    "            if aid < len(self.articles):\n",
    "                tags = self.articles.iloc[aid]['tags']\n",
    "                # Handle both string and list representations of tags\n",
    "                if isinstance(tags, str):\n",
    "                    try:\n",
    "                        tags = eval(tags)  # Safely convert string representation to list\n",
    "                    except:\n",
    "                        tags = [tags]\n",
    "                for tag in tags:\n",
    "                    tag_counts[tag] += 1\n",
    "                    \n",
    "        favorite_tags = sorted(\n",
    "            tag_counts.items(), \n",
    "            key=lambda x: x[1], \n",
    "            reverse=True\n",
    "        )[:5]\n",
    "        \n",
    "        return {\n",
    "            'total_interactions': sum(interaction_counts.values()),\n",
    "            'favorite_tags': [tag for tag, _ in favorite_tags],\n",
    "            'interaction_counts': interaction_counts\n",
    "        }\n",
    "    \n",
    "    def get_trending_articles(self, timeframe_hours: int = 24, n_articles: int = 5) -> List[Dict]:\n",
    "        \"\"\"Get trending articles based on recent interactions\"\"\"\n",
    "        current_time = datetime.now()\n",
    "        \n",
    "        # Initialize trending scores as numpy array\n",
    "        trending_scores = np.zeros(len(self.articles))\n",
    "        \n",
    "        # Calculate trending scores based on recent interactions\n",
    "        for int_type in InteractionTypes:\n",
    "            if int_type != InteractionTypes.NOT_INTERESTED:\n",
    "                matrix = self.interaction_matrices[int_type]\n",
    "                weight = self.interaction_weights[int_type]\n",
    "                \n",
    "                # Sum up weighted interactions and ensure numpy array format\n",
    "                article_interactions = np.array(matrix.sum(axis=0)).flatten()\n",
    "                trending_scores += article_interactions * weight\n",
    "        \n",
    "        # Get top trending articles\n",
    "        trending_ids = np.argsort(trending_scores)[::-1][:n_articles]\n",
    "        \n",
    "        trending_articles = []\n",
    "        for idx in trending_ids:\n",
    "            trending_articles.append({\n",
    "                'article_id': int(idx),\n",
    "                'title': self.articles.iloc[idx]['title'],\n",
    "                'trending_score': float(trending_scores[idx]),\n",
    "                'tags': self.articles.iloc[idx]['tags']\n",
    "            })\n",
    "            \n",
    "        return trending_articles\n",
    "    def get_similar_articles(\n",
    "        self, \n",
    "        article_id: int, \n",
    "        n_recommendations: int = 5,\n",
    "        include_boosted: bool = True,\n",
    "        similarity_threshold: float = 0.1\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Get similar articles based on content and interaction patterns\n",
    "        \n",
    "        Args:\n",
    "            article_id: ID of the article to find similarities for\n",
    "            n_recommendations: number of similar articles to return\n",
    "            include_boosted: whether to include boosted articles\n",
    "            similarity_threshold: minimum similarity score to include\n",
    "            \n",
    "        Returns:\n",
    "            List of similar articles with similarity details\n",
    "        \"\"\"\n",
    "        # Get content-based similarity scores\n",
    "        content_similarities = self.content_similarities[article_id]\n",
    "        \n",
    "        # Get topic-based similarity scores\n",
    "        topic_similarities = cosine_similarity(\n",
    "            self.topic_features[article_id:article_id+1], \n",
    "            self.topic_features\n",
    "        )[0]\n",
    "        \n",
    "        # Get interaction-based similarity if we have collaborative data\n",
    "        interaction_similarities = np.zeros(len(self.articles))\n",
    "        if self.collaborative_model is not None:\n",
    "            article_vector = self.article_embeddings[article_id]\n",
    "            interaction_similarities = np.dot(self.article_embeddings, article_vector)\n",
    "        \n",
    "        # Combine similarity scores\n",
    "        final_similarities = (\n",
    "            0.4 * content_similarities +\n",
    "            0.4 * topic_similarities +\n",
    "            0.2 * interaction_similarities\n",
    "        )\n",
    "        \n",
    "        # Apply boosting if requested\n",
    "        if include_boosted:\n",
    "            boost_scores = np.array([\n",
    "                self._get_article_boost_score(aid) \n",
    "                for aid in range(len(self.articles))\n",
    "            ])\n",
    "            final_similarities *= boost_scores\n",
    "        \n",
    "        # Filter by threshold and get top articles\n",
    "        final_similarities[final_similarities < similarity_threshold] = 0\n",
    "        final_similarities[article_id] = 0  # Exclude the input article\n",
    "        similar_ids = np.argsort(final_similarities)[::-1][:n_recommendations]\n",
    "        \n",
    "        similar_articles = []\n",
    "        for idx in similar_ids:\n",
    "            if final_similarities[idx] > 0:\n",
    "                similar_articles.append({\n",
    "                    'article_id': idx,\n",
    "                    'title': self.articles.iloc[idx]['title'],\n",
    "                    'overall_similarity': float(final_similarities[idx]),\n",
    "                    'content_similarity': float(content_similarities[idx]),\n",
    "                    'topic_similarity': float(topic_similarities[idx]),\n",
    "                    'interaction_similarity': float(interaction_similarities[idx]),\n",
    "                    'tags': self.articles.iloc[idx]['tags'],\n",
    "                    'shared_tags': list(\n",
    "                        set(self.articles.iloc[idx]['tags']) & \n",
    "                        set(self.articles.iloc[article_id]['tags'])\n",
    "                    ),\n",
    "                    'boost_score': float(self._get_article_boost_score(idx)),\n",
    "                    'is_boosted': self._get_article_boost_score(idx) > 1.0\n",
    "                })\n",
    "        \n",
    "        return similar_articles\n",
    "\n",
    "\n",
    "    def get_articles_by_tag(\n",
    "        self,\n",
    "        tag: str,\n",
    "        n_articles: int = 5,\n",
    "        include_boosted: bool = True\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"Get articles by tag with smart ranking\"\"\"\n",
    "        matching_articles = []\n",
    "        for idx, article in self.articles.iterrows():\n",
    "            if tag in article['tags']:\n",
    "                score = 1.0\n",
    "                \n",
    "                # Parse timestamp from string if needed\n",
    "                try:\n",
    "                    article_timestamp = article['timestamp']\n",
    "                    if isinstance(article_timestamp, str):\n",
    "                        article_timestamp = datetime.fromisoformat(article_timestamp.replace('Z', '+00:00'))\n",
    "                    \n",
    "                    # Factor in recency \n",
    "                    days_old = (datetime.now(article_timestamp.tzinfo) - article_timestamp).days\n",
    "                    recency_score = 1.0 / (1.0 + days_old/30)  # Decay over 30 days\n",
    "                    score *= recency_score\n",
    "                except (ValueError, TypeError, AttributeError):\n",
    "                    # If timestamp parsing fails, use neutral recency score\n",
    "                    score *= 1.0\n",
    "                \n",
    "                # Factor in interaction counts if available\n",
    "                if self.collaborative_model is not None:\n",
    "                    interaction_score = sum(\n",
    "                        matrix[:, idx].sum() * weight\n",
    "                        for matrix, weight in zip(\n",
    "                            self.interaction_matrices.values(),\n",
    "                            self.interaction_weights.values()\n",
    "                        )\n",
    "                    )\n",
    "                    score *= (1.0 + interaction_score)\n",
    "                \n",
    "                # Apply boosting if requested\n",
    "                boost_score = 1.0\n",
    "                if include_boosted:\n",
    "                    boost_score = self._get_article_boost_score(idx)\n",
    "                    score *= boost_score\n",
    "                \n",
    "                matching_articles.append({\n",
    "                    'article_id': idx,\n",
    "                    'title': article['title'],\n",
    "                    'tags': article['tags'],\n",
    "                    'score': float(score),\n",
    "                    'timestamp': article['timestamp'],\n",
    "                    'is_boosted': boost_score > 1.0 if include_boosted else False\n",
    "                })\n",
    "        \n",
    "        # Sort by score and return top n\n",
    "        matching_articles.sort(key=lambda x: x['score'], reverse=True)\n",
    "        return matching_articles[:n_articles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/medium_articles_reduced.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_article_details(article_id: int):\n",
    "    article = df.iloc[article_id]\n",
    "    print(f\"Title: {article['title']}\")\n",
    "    print(f\"Text: {article['text']}\")\n",
    "    print(f\"URL: {article['url']}\")\n",
    "    print(f\"Authors: {article['authors']}\")\n",
    "    print(f\"Timestamp: {article['timestamp']}\")\n",
    "    print(f\"Tags: {article['tags']}\")\n",
    "\n",
    "# Example usage\n",
    "print_article_details(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommender = EnhancedHybridRecommender(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommender.add_user_interaction(user_id=1, article_id=42, interaction_type=\"view\")\n",
    "recommender.add_user_interaction(user_id=1, article_id=42, interaction_type=\"like\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import  timedelta\n",
    "boost = ArticleBoost(\n",
    "    article_id=42,\n",
    "    boost_factor=1.5,\n",
    "    start_time=datetime.now(),\n",
    "    end_time=datetime.now() + timedelta(days=7),\n",
    "    boost_type='sponsored'\n",
    ")\n",
    "recommender.add_article_boost(boost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_recommendations = recommender.get_recommendations(user_id=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile = recommender.get_user_profile(user_id=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trending = recommender.get_trending_articles(timeframe_hours=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_articles = recommender.get_similar_articles(\n",
    "    article_id=42,\n",
    "    n_recommendations=5,\n",
    "    include_boosted=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_articles = recommender.get_articles_by_tag(\n",
    "    tag=\"technology\",\n",
    "    n_articles=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('data/medium_articles.csv')\n",
    "df_reduced = df.sample(n=10000, random_state=42)\n",
    "df_reduced.to_csv('data/medium_articles_reduced.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
